<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Apache Airflow – Use cases</title>
    <link>/use-cases/</link>
    <description>Recent content in Use cases on Apache Airflow</description>
    <generator>Hugo -- gohugo.io</generator>
    
	  <atom:link href="/use-cases/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>Use-Cases: Adobe</title>
      <link>/use-cases/adobe/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/use-cases/adobe/</guid>
      <description>
        
        
        

&lt;h5 id=&#34;what-was-the-problem&#34;&gt;What was the problem?&lt;/h5&gt;

&lt;p&gt;Modern big data platforms need sophisticated data pipelines connecting to many backend services enabling complex workflows. These workflows need to be deployed, monitored, and run either on regular schedules or triggered by external events. Adobe Experience Platform component services architected and built an orchestration service to enable their users to author, schedule, and monitor complex hierarchical (including sequential and parallel) workflows for Apache Spark &amp;trade; and non-Spark jobs.&lt;/p&gt;

&lt;h5 id=&#34;how-did-apache-airflow-help-to-solve-this-problem&#34;&gt;How did Apache Airflow help to solve this problem?&lt;/h5&gt;

&lt;p&gt;Adobe Experience Platform built an orchestration service to meet our user and customer requirements. It is architected based on guiding principles to leverage an off-the-shelf, open-source orchestration engine that is abstracted to other services through an API and extendible to any application through a pluggable framework. Adobe Experience Platform orchestration service leverages Apache Airflow execution engine for scheduling and executing various workflows. Apache Airflow is highly extensible and with support of K8s Executor it can scale to meet our requirements. It has a very rich Airflow Web UI to provide various workflow-related insights. Airflow’s active community that addresses issues and different feature requests also made it additionally attractive for us.&lt;/p&gt;

&lt;h5 id=&#34;what-are-the-results&#34;&gt;What are the results?&lt;/h5&gt;

&lt;p&gt;Adobe Experience Platform is using Apache Airflow&amp;rsquo;s plugin interface to write custom operators to meet our use cases. With K8s Executor, we could scale it to run 1000(s) of concurrent workflows. Adobe and Adobe Experience Platform teams can focus on business use cases because all scheduling, dependency management, and retrying logic is offloaded to Apache Airflow.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Use-Cases: Big Fish Games</title>
      <link>/use-cases/big-fish-games/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/use-cases/big-fish-games/</guid>
      <description>
        
        
        

&lt;h5 id=&#34;what-was-the-problem&#34;&gt;What was the problem?&lt;/h5&gt;

&lt;p&gt;The main challenge is the lack of standardized  ETL workflow orchestration tools. PowerShell and Python-based ETL frameworks built in-house are currently used for scheduling and running analytical workloads. However, there is no web UI through which we can monitor these workflows and it requires additional effort to maintain this framework. These scheduled jobs based on external dependencies are not well suited to modern Big Data platforms and their complex workflows. Although we experimented with Apache Oozie for certain workflows, it did not handle failed jobs properly. For late data arrival, these tools are not flexible enough to enforce retry attempts for the job failures.&lt;/p&gt;

&lt;h5 id=&#34;how-did-apache-airflow-help-to-solve-this-problem&#34;&gt;How did Apache Airflow help to solve this problem?&lt;/h5&gt;

&lt;p&gt;Apache Airflow helps us programmatically control our workflows in Python by setting task dependencies and monitoring tasks within each DAG in a Web UI. Airflow allows us to view detailed logs for each task in these complex workflows. It has built-in connectors for Hive, MySQL, Google Cloud APIs and others. It also lends us flexibility to create our own custom connectors (i.e. for a Netezza database) using JDBCHook and JDBCOperator or extend the existing operator such as Hive Operator. For complex workflows, we can design ETLs using Airflow by running certain tasks only on weekdays. A powerful feature of Airflow is its support for backfilling of data: when we add a new task to a DAG, we can backfill for that task alone. Airflow also allows us to set external DAG dependencies alongside features such as SQLSensor on a database table to run a specific task.&lt;/p&gt;

&lt;h5 id=&#34;what-are-the-results&#34;&gt;What are the results?&lt;/h5&gt;

&lt;p&gt;We seek to run concurrent tasks with DAGs and concurrent DAGs using Apache Airflow, in hopes of running our entire ETL workload faster. Airflow helps our analysts and developers focus on the analyses, rather than labor over building an ETL framework to schedule and monitor our applications. Airflow facilitates a seamless ETL migration to the Google Cloud Platform (GCP), as GCP maintains Cloud Composer, an Apache Airflow managed service.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Use-Cases: Dish</title>
      <link>/use-cases/dish/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/use-cases/dish/</guid>
      <description>
        
        
        

&lt;h5 id=&#34;what-was-the-problem&#34;&gt;What was the problem?&lt;/h5&gt;

&lt;p&gt;We faced increasing complexity managing lengthy crontabs with scheduling being an issue, this required carefully planning timing due to resource constraints, usage patterns, and especially custom code needed for retry logic.  In the last case, having to verify success of previous jobs and/or steps prior to running the next.  Furthermore, time to results is important, but we were increasingly relying on buffers for processing, where things were effectively sitting idle and not processing, waiting for the next stage, in an effort to not rely as much on custom code/logic.&lt;/p&gt;

&lt;h5 id=&#34;how-did-apache-airflow-help-to-solve-this-problem&#34;&gt;How did Apache Airflow help to solve this problem?&lt;/h5&gt;

&lt;p&gt;Relying on community built and existing hooks and operators to the majority of cloud services we use has allowed us to focus on business outcomes rather than operations.&lt;/p&gt;

&lt;h5 id=&#34;what-are-the-results&#34;&gt;What are the results?&lt;/h5&gt;

&lt;p&gt;Airflow helps us manage many of our pain-points, letting us benefit from the overall ecosystem and community.  We are able to reduce time-to-end delivery of data products by being event-driven in our processing flows (in our first usage, for example, we were able to take out over 2 hours - on average - of various waiting between stages).  Furthermore, we are able to arrive at and iterate on products quicker as a result of not needing as much custom or roll-our-own solutions.  For Our code base is smaller and simpler, it is easier to follow, and to a large extent our DAGs serve as sufficient documentation for new contributors to understand what is going on.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Use-Cases: Experity</title>
      <link>/use-cases/experity/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/use-cases/experity/</guid>
      <description>
        
        
        

&lt;h5 id=&#34;what-was-the-problem&#34;&gt;What was the problem?&lt;/h5&gt;

&lt;p&gt;We had to deploy our complex, flagship app to multiple nodes in multiple ways. This required tasks to communicate across Windows nodes and coordinate timing perfectly. We did not want to buy an expensive enterprise scheduling tool and needed ultimate flexibility.&lt;/p&gt;

&lt;h5 id=&#34;how-did-apache-airflow-help-to-solve-this-problem&#34;&gt;How did Apache Airflow help to solve this problem?&lt;/h5&gt;

&lt;p&gt;Ultimately we decided flexible, multi-node, DAG capable tooling was key and airflow was one of the few tools that fit that bill. Having it based on open source and python were large factors that upheld our core principles. At the time, Airflow was missing a windows hook and operator so we contributed the WinRM hook and operator back to the community. Given its flexibilty we also use DAG generators to have our metadata drive our DAGs and keep maintenance costs down.&lt;/p&gt;

&lt;h5 id=&#34;what-are-the-results&#34;&gt;What are the results?&lt;/h5&gt;

&lt;p&gt;We have a very flexible deployment framework that allows us to be as nimble as possible. The reliability is something we have grown to trust as long as we use the tool correctly. The scalability has also allowed us to decrease the time it takes to operate on our fleet of servers.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Use-Cases: Onefootball</title>
      <link>/use-cases/onefootball/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/use-cases/onefootball/</guid>
      <description>
        
        
        

&lt;h5 id=&#34;what-was-the-problem&#34;&gt;What was the problem?&lt;/h5&gt;

&lt;p&gt;With millions of daily active users, managing the complexity of data engineering at Onefootball is a constant challenge. Lengthy crontabs, multiplication of custom API clients, erosion of confidence in the analytics served, increasing heroism (&amp;ldquo;only one person can solve this issue&amp;rdquo;). Those are the challenges that most teams face unless they consciously invest in their tools and processes.&lt;/p&gt;

&lt;p&gt;On top of that, new data tools appear each month: third party data sources, cloud providers solutions, different storage technologies&amp;hellip; Managing all those integrations is costly and brittle, especially for small data engineering teams that are trying to do more with less.&lt;/p&gt;

&lt;h5 id=&#34;how-did-apache-airflow-help-to-solve-this-problem&#34;&gt;How did Apache Airflow help to solve this problem?&lt;/h5&gt;

&lt;p&gt;Airflow had been on our radar for a while until one day we took the leap. We used the DAG paradigm to migrate the pipelines running on crontabs. We benefited from the community Hooks and Operators to remove parts of our code, or to refactor the API clients specific to our business. We use the alerts, SLAs and the web UI to regain confidence in our analytics. We use our airflow internal PRs as catalysts for team discussion and to challenge our technical designs.&lt;/p&gt;

&lt;p&gt;We have DAGs orchestrating SQL transformations in our data warehouse, but also DAGs that are orchestrating functions ran against our Kubernetes cluster both for training Machine Learning models and sending daily analytics emails.&lt;/p&gt;

&lt;h5 id=&#34;what-are-the-results&#34;&gt;What are the results?&lt;/h5&gt;

&lt;p&gt;The learning curve was steep but in about 100 days we were able to efficiently use Airflow to manage the complexity of our data engineering. We currently have 17 DAGs (adding on average 1 per week), we have 2 contributions on apache/airflow, we have 7 internal hooks and operators and are planning to add more as our migration efforts continue.&lt;/p&gt;

      </description>
    </item>
    
  </channel>
</rss>
